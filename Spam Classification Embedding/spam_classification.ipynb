{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0d0ba85-9944-4e87-80f5-30a75e17d7a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLASS</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>said kiss, kiss, i can't do the sound effects...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>&amp;lt;#&amp;gt; ISH MINUTES WAS 5 MINUTES AGO. WTF.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>(Bank of Granite issues Strong-Buy) EXPLOSIVE ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>* FREE* POLYPHONIC RINGTONE Text SUPER to 8713...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spam</td>\n",
       "      <td>**FREE MESSAGE**Thanks for using the Auction S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CLASS                                                SMS\n",
       "0   ham   said kiss, kiss, i can't do the sound effects...\n",
       "1   ham      &lt;#&gt; ISH MINUTES WAS 5 MINUTES AGO. WTF.\n",
       "2  spam  (Bank of Granite issues Strong-Buy) EXPLOSIVE ...\n",
       "3  spam  * FREE* POLYPHONIC RINGTONE Text SUPER to 8713...\n",
       "4  spam  **FREE MESSAGE**Thanks for using the Auction S..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "spam_data = pd.read_csv(\"Spam-Classification.csv\")\n",
    "spam_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ad87a04-4494-4bb3-89a1-7c38f5d43d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate feature and target\n",
    "spam_classes_raw = spam_data[\"CLASS\"]\n",
    "spam_messages = spam_data[\"SMS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ace7787f-229a-44d6-9952-d7efa7767623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding :  (1500, 2)\n"
     ]
    }
   ],
   "source": [
    "# preprocessing data\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "spam_classes = label_encoder.fit_transform(spam_classes_raw)\n",
    "\n",
    "# convert target to one-hot encoding\n",
    "spam_classes = tf.keras.utils.to_categorical(spam_classes,2)\n",
    "print(\"One-hot encoding : \", spam_classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48810d65-3884-4f86-860c-0cb2b9cfa3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens found:  4688\n",
      "Example token ID for word \"me\" : 25\n",
      "\n",
      "Total sequences found :  1500\n",
      "Example Sequence for sentence :   said kiss, kiss, i can't do the sound effects! He is a gorgeous man isn't he! Kind of person who needs a smile to brighten his day! \n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0  260  921  921    4  430   55    6 1488 2294  148   10\n",
      "    3 1489  464 1143  148  922   19  514   77 1144    3  515    1 2295\n",
      "  397   89]\n"
     ]
    }
   ],
   "source": [
    "#Preprocess data for spam messages\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#Max words in the vocabulary for this dataset\n",
    "VOCAB_WORDS=10000\n",
    "#Max sequence length for word sequences\n",
    "MAX_SEQUENCE_LENGTH=100\n",
    "\n",
    "#Create a vocabulary with unique words and IDs\n",
    "spam_tokenizer = Tokenizer(num_words=VOCAB_WORDS)\n",
    "spam_tokenizer.fit_on_texts(spam_messages)\n",
    "\n",
    "\n",
    "print(\"Total unique tokens found: \", len(spam_tokenizer.word_index))\n",
    "print(\"Example token ID for word \\\"me\\\" :\", spam_tokenizer.word_index.get(\"me\"))\n",
    "\n",
    "#Convert sentences to token-ID sequences\n",
    "spam_sequences = spam_tokenizer.texts_to_sequences(spam_messages)\n",
    "\n",
    "#Pad all sequences to fixed length\n",
    "spam_padded = pad_sequences(spam_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print(\"\\nTotal sequences found : \", len(spam_padded))\n",
    "print(\"Example Sequence for sentence : \", spam_messages[0] )\n",
    "print(spam_padded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8b0e60e-7a21-477c-b266-656c5ddc1978",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Split into training and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "                                    spam_padded,spam_classes,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce33951a-f9f0-439b-964a-0ded64778ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary Size:  400000\n",
      "\n",
      " Sample Dictionary Entry for word \"the\" :\n",
      " [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n"
     ]
    }
   ],
   "source": [
    "#Load the pre-trained embeddings\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#Read pretrained embeddings into a dictionary\n",
    "glove_dict = {} \n",
    "\n",
    "#Loading a 50 feature (dimension) embedding with 6 billion words\n",
    "with open('glove.6B.50d.txt/glove.6B.50d.txt', \"r\", encoding=\"utf8\") as glove_file:     \n",
    "    for line in glove_file:\n",
    "        \n",
    "        emb_line = line.split()      \n",
    "        emb_token = emb_line[0]         \n",
    "        emb_vector = np.array(emb_line[1:], dtype=np.float32)\n",
    "        \n",
    "        if emb_vector.shape[0] == 50:    \n",
    "            glove_dict[emb_token] = emb_vector \n",
    "\n",
    "print(\"Dictionary Size: \", len(glove_dict))\n",
    "print(\"\\n Sample Dictionary Entry for word \\\"the\\\" :\\n\", glove_dict.get(\"the\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a9df36e-442b-40ea-8518-660a044133bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Embedding matrix : (4689, 50)\n",
      "Embedding Vector for word \"me\" : \n",
      " [-0.14524999  0.31265     0.15184    -0.63708001  0.63552999 -0.50295001\n",
      " -0.23214     0.52891999 -0.58629     0.53934997 -0.3055      1.03569996\n",
      " -0.77989    -0.19386999  1.22150004  0.24521001  0.26144001  0.22439\n",
      "  0.15583999 -0.79145998 -0.65262002  1.3211      0.76617998  0.38234001\n",
      "  1.44529998 -2.26430011 -1.15050006  0.50373     1.2651     -1.59029996\n",
      "  3.05180001  0.84118003 -0.69542998  0.29984999 -0.49151    -0.22312\n",
      "  0.59527999 -0.076347    0.52358001 -0.50133997  0.22483     0.01546\n",
      " -0.088005    0.21281999  0.28545001 -0.15976    -0.16777    -0.50895\n",
      "  0.14322001  1.01180005]\n"
     ]
    }
   ],
   "source": [
    "#We now associate each token ID in our data set vocabulary to the corresponding embedding in Glove\n",
    "#If the word is not available, then embedding will be all zeros.\n",
    "\n",
    "#Matrix with 1 row for each word in the data set vocubulary and 50 features\n",
    "\n",
    "vocab_len = len(spam_tokenizer.word_index) + 1\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_len, 50))\n",
    "\n",
    "for word, id in spam_tokenizer.word_index.items():  \n",
    "    try:\n",
    "        embedding_vector = glove_dict.get(word) \n",
    "        if embedding_vector is not None:         \n",
    "            embedding_matrix[id] = embedding_vector\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"Size of Embedding matrix :\", embedding_matrix.shape)\n",
    "print(\"Embedding Vector for word \\\"me\\\" : \\n\", embedding_matrix[spam_tokenizer.word_index.get(\"me\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "050f6181-eaeb-4c43-b6e5-5ab40d9b5413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Embedding-Layer (Embedding  (None, 100, 50)           234450    \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 256)               314368    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " Output-Layer (Dense)        (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 549332 (2.10 MB)\n",
      "Trainable params: 549332 (2.10 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Create a model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras.layers import LSTM,Dense\n",
    "\n",
    "#Setup Hyper Parameters for building the model\n",
    "NB_CLASSES=2\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Embedding(vocab_len,\n",
    "                                 50, \n",
    "                                 name=\"Embedding-Layer\",\n",
    "                                 weights=[embedding_matrix],\n",
    "                                 input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                 trainable=True))\n",
    "\n",
    "#Add LSTM Layer\n",
    "model.add(LSTM(256))\n",
    "model.add(keras.layers.Flatten())\n",
    "\n",
    "model.add(keras.layers.Dense(NB_CLASSES,\n",
    "                             name='Output-Layer',\n",
    "                             activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39bd1a1c-ba63-4f6e-ac2f-069226eb6007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Progress:\n",
      "------------------------------------\n",
      "Epoch 1/10\n",
      "4/4 [==============================] - 8s 1s/step - loss: 0.6159 - accuracy: 0.6010 - val_loss: 0.3804 - val_accuracy: 0.8750\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.2869 - accuracy: 0.9073 - val_loss: 0.5514 - val_accuracy: 0.7417\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.2875 - accuracy: 0.8969 - val_loss: 0.1947 - val_accuracy: 0.9250\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.1686 - accuracy: 0.9417 - val_loss: 0.1805 - val_accuracy: 0.9375\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.1993 - accuracy: 0.9354 - val_loss: 0.1603 - val_accuracy: 0.9458\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.1417 - accuracy: 0.9531 - val_loss: 0.2616 - val_accuracy: 0.9167\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.1627 - accuracy: 0.9458 - val_loss: 0.1543 - val_accuracy: 0.9417\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 5s 1s/step - loss: 0.1514 - accuracy: 0.9406 - val_loss: 0.2730 - val_accuracy: 0.8833\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.1476 - accuracy: 0.9490 - val_loss: 0.1491 - val_accuracy: 0.9375\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 4s 1s/step - loss: 0.1022 - accuracy: 0.9677 - val_loss: 0.1781 - val_accuracy: 0.9250\n",
      "\n",
      "Evaluation against Test Dataset :\n",
      "------------------------------------\n",
      "10/10 [==============================] - 1s 111ms/step - loss: 0.1584 - accuracy: 0.9600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15836729109287262, 0.9599999785423279]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make it verbose so we can see the progress\n",
    "VERBOSE=1\n",
    "\n",
    "#Setup Hyper Parameters for training\n",
    "BATCH_SIZE=256\n",
    "EPOCHS=10\n",
    "VALIDATION_SPLIT=0.2\n",
    "\n",
    "print(\"\\nTraining Progress:\\n------------------------------------\")\n",
    "\n",
    "history=model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          verbose=VERBOSE,\n",
    "          validation_split=VALIDATION_SPLIT)\n",
    "\n",
    "print(\"\\nEvaluation against Test Dataset :\\n------------------------------------\")\n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34d68d96-b294-4c6a-82b7-2494f37aef06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 603ms/step\n",
      "Prediction Output: [1 0]\n",
      "Prediction Classes are  ['spam' 'ham']\n"
     ]
    }
   ],
   "source": [
    "# Two input strings to predict\n",
    "input_str=[\"Unsubscribe send GET EURO STOP to 83222\",\n",
    "            \"Yup I will come over\"]\n",
    "\n",
    "#Convert to sequence using the same tokenizer as training\n",
    "input_seq = spam_tokenizer.texts_to_sequences(input_str)\n",
    "#Pad the input\n",
    "input_padded = pad_sequences(input_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "#Predict using model\n",
    "prediction=np.argmax( model.predict(input_padded), axis=1 )\n",
    "print(\"Prediction Output:\" , prediction)\n",
    "\n",
    "#Print prediction classes\n",
    "print(\"Prediction Classes are \", label_encoder.inverse_transform(prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
